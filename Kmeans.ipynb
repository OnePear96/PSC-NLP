{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "import spacy\n",
    "frlemma = spacy.load('fr')\n",
    "\n",
    "\n",
    "def get_data(path, filename, comlumname):\n",
    "    print('get data begin')\n",
    "    df = pd.read_json(path + filename, orient='values')\n",
    "    data = df[comlumname].astype(str).values.tolist()\n",
    "    print('got it')\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "\n",
    "    raw_stopword_list = stopwords.words('french')\n",
    "    stopword_list = [word for word in raw_stopword_list]\n",
    "    for w in ['les', 'aussi','comme','plus','deux','trois','très','cette','accélérateur','jeune','pousse','fablab','entreprises','studio','couveuse','incubateur','coworking','co-working'\n",
    "              ,'entreprise','startup','start-up','start-ups','startups','être','pépinière','tout','faire','incubator','ils','oui','elles','non']:\n",
    "        stopword_list.append(w)\n",
    "    return stopword_list\n",
    "\n",
    "def cleandata(data, stopwords):\n",
    "    print('cleaning data begins')\n",
    "    newdata = []\n",
    "    print('number of document',len(data))\n",
    "    i = 0\n",
    "    for a in data:\n",
    "        if i%1000 == 0:\n",
    "           print('now in the', i)\n",
    "        newdata.append(delete_stopwords(a, stopwords))\n",
    "        i = i +1\n",
    "    print('clean finish')\n",
    "    return newdata\n",
    "\n",
    "def delete_stopwords(raw, stopwords,encoding = 'utf-8'):\n",
    "    no_commas = re.sub(r'[.|,|\\']', ' ', raw)\n",
    "    tokens = nltk.word_tokenize(no_commas)\n",
    "    wordlist =[w.lower() for w in tokens]\n",
    "    filtered_words = []\n",
    "    #stemmer = SnowballStemmer(\"french\")\n",
    "    #lemmer = FrenchLefffLemmatizer()\n",
    "\n",
    "    for item in wordlist:\n",
    "        if item not in stopwords and item.isalpha() and len(item) > 1:\n",
    "            #filtered_words.append(item)\n",
    "            #filtered_words.append(stemmer.stem(item))\n",
    "            filtered_words.append(frlemma(item)[0].lemma_)\n",
    "    line = ''\n",
    "    for word in filtered_words:\n",
    "        \n",
    "        if word.encode('utf-8') == '\\n' or word.encode('utf-8') == 'nbsp' or word.encode('utf-8') == '\\r\\n':\n",
    "            continue\n",
    "        line += word\n",
    "        line += ' '\n",
    "    return line.strip()\n",
    "\n",
    "def tfidf(corpus):\n",
    "    print('tfidf begin')\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    transformer = TfidfTransformer()\n",
    "\n",
    "    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "\n",
    "    #print(tfidf)\n",
    "    words = vectorizer.get_feature_names()\n",
    "\n",
    "    weight = tfidf.toarray()\n",
    "    #pprint(word)\n",
    "    print(weight)\n",
    "\n",
    "    return weight,words\n",
    "\n",
    "def lsa(weight, n):\n",
    "    print('begin lsa')\n",
    "    svd = TruncatedSVD(n_components=n)\n",
    "    normalizer = Normalizer(copy= False)\n",
    "    lsa = make_pipeline(svd,normalizer)\n",
    "    X = lsa.fit_transform(weight)\n",
    "    print('lsa end')\n",
    "    return  X, svd\n",
    "\n",
    "def pca(weight, dimension):\n",
    "    print('原有维度: ', len(weight[0]))\n",
    "    print('开始降维:')\n",
    "\n",
    "    pca = PCA(n_components=dimension)  # 初始化PCA\n",
    "    X = pca.fit_transform(weight)  # 返回降维后的数据\n",
    "    print('降维后维度: ', len(X[0]))\n",
    "    print(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def kmeans(X, k, words):  # X=weight\n",
    "\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    print ('cluser begin for k = ', k)\n",
    "    \n",
    "    clusterer = KMeans(n_clusters=k, init='k-means++')  # 设置聚类模型\n",
    "\n",
    "    y = clusterer.fit_predict(X)  # 把weight矩阵扔进去fit一下,输出label\n",
    "    #print('kmeans labels:'+clusterer.labels_)\n",
    "    print('kmeans inertia:', clusterer.inertia_)\n",
    "    order_centroids = clusterer.cluster_centers_.argsort()[:, ::-1]\n",
    "        \n",
    "    for i in range(k):\n",
    "        top_ten_words = [words[ind] for ind in order_centroids[i, :15]]\n",
    "        print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))\n",
    "\n",
    "    print('kmeans catalogy:', y)\n",
    "    return y\n",
    "\n",
    "def Silhouette(X, y):\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "    print ('silhouette score:')\n",
    "\n",
    "    silhouette_avg = silhouette_score(X, y)  # 平均轮廓系数\n",
    "    sample_silhouette_values = silhouette_samples(X, y)  # 每个点的轮廓系数\n",
    "\n",
    "    pprint(silhouette_avg)\n",
    "\n",
    "    return silhouette_avg, sample_silhouette_values\n",
    "\n",
    "def Draw(silhouette_avg, sample_silhouette_values, y, k):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "    import numpy as np\n",
    "\n",
    "    # 创建一个 subplot with 1-row 2-column\n",
    "    fig, ax1 = plt.subplots(1)\n",
    "#     fig.set_size_inches(18, 7)\n",
    "\n",
    "    # 第一个 subplot 放轮廓系数点\n",
    "    # 范围是[-1, 1]\n",
    "    ax1.set_xlim([-1, 1])\n",
    "\n",
    "    # 后面的 (k + 1) * 10 是为了能更明确的展现这些点\n",
    "    ax1.set_ylim([0, len(y) + (k + 1) * 10])\n",
    "\n",
    "    y_lower = 10\n",
    "\n",
    "    for i in range(k):  # 分别遍历这几个聚类\n",
    "\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[y == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.Spectral(float(i) / k)  # 搞一款颜色\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0,\n",
    "                          ith_cluster_silhouette_values,\n",
    "                          facecolor=color,\n",
    "                          edgecolor=color,\n",
    "                          alpha=0.7)  # 这个系数不知道干什么的\n",
    "\n",
    "        # 在轮廓系数点这里加上聚类的类别号\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # 计算下一个点的 y_lower y轴位置\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    # 在图里搞一条垂直的评论轮廓系数虚线\n",
    "  #  ax1.axvline(x=silhouette_avg, color='red', linestyle=\"--\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def NewDraw(sample_silhouette_values,y,k):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "    import numpy as np\n",
    "    # 创建一个 subplot with 1-row 2-column\n",
    "    fig, ax1 = plt.subplots(1)\n",
    "    fig.set_size_inches(7, 5)\n",
    "\n",
    "    # 第一个 subplot 放轮廓系数点\n",
    "    # 范围是[-1, 1]\n",
    "#     ax1.set_xlim([0, 1])\n",
    "\n",
    "    # 后面的 (k + 1) * 10 是为了能更明确的展现这些点\n",
    "\n",
    "\n",
    "    for i in range(k):  # 分别遍历这几个聚类\n",
    "\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[y == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        \n",
    "\n",
    "        color = cm.Spectral(float(i) / k)  # 搞一款颜色\n",
    "       \n",
    "        ax1.bar(i, size_cluster_i, align='center', \n",
    "                #log = 'false',\n",
    "                color = color)\n",
    "       # plt.xticks(i, str(i))\n",
    "#          在轮廓系数点这里加上聚类的类别号\n",
    "        ax1.text(i-0.25, size_cluster_i + 10, str(size_cluster_i))\n",
    "\n",
    "#         # 计算下一个点的 y_lower y轴位置\n",
    "#         y_lower = y_upper + 10\n",
    "\n",
    "    # 在图里搞一条垂直的评论轮廓系数虚线\n",
    "  #  ax1.axvline(x=silhouette_avg, color='red', linestyle=\"--\")\n",
    "    plt.title('the quantity of document for each topic')\n",
    "    plt.ylabel('Nr of documents')\n",
    "    plt.xlabel('Topics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get weight marix and the list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "filename = \"articles_Accuracy.json\"\n",
    "comlumname = \"body_fr\"\n",
    "data = get_data(path, filename, comlumname)\n",
    "    #corpus = [ 'je suis ton Père. Elle est ta mère. ', 'Sunny day weather is suitable to exercise','I ate a Hotdog']\n",
    "\n",
    "frenchstopwords = get_stopwords()\n",
    "frenchstopwords.append(w for w in ['les', 'aussi','comme','plus','deux','trois','très','cette','ils','ça','entre'])\n",
    "cleaned_data= cleandata(data, frenchstopwords)\n",
    "weight,words = tfidf(cleaned_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in  range(8, 12):\n",
    "    y = kmeans(weight,k,words)\n",
    "    silhouette_avg, sample_silhouette_values = Silhouette(weight, y)\n",
    "    Draw(silhouette_avg, sample_silhouette_values, y, k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, svd = lsa(weight, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortd = kmeans(b, 10, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg, sample_silhouette_values = Silhouette(b, shortd)\n",
    "Draw(silhouette_avg, sample_silhouette_values, shortd, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsakmeans(weight, k, ncompinents, words):\n",
    "    from sklearn.cluster import KMeans\n",
    "    svd = TruncatedSVD(n_components= ncompinents)\n",
    "    normalizer = Normalizer(copy= False)\n",
    "    lsa = make_pipeline(svd,normalizer)\n",
    "    X = lsa.fit_transform(weight)\n",
    "    km = KMeans(n_clusters=k, init='k-means++')  # 设置聚类模型\n",
    "    y = km.fit_predict(X)  # 把weight矩阵扔进去fit一下,输出label\n",
    "        #print('kmeans labels:'+clusterer.labels_)\n",
    "    print('kmeans inertia:', km.inertia_)\n",
    "    original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    for i in range(k):\n",
    "            top_ten_words = [words[ind] for ind in order_centroids[i, :15]]\n",
    "            print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(8, 15):\n",
    "    newweight, y  = lsakmeans(weight, k, 500, words)\n",
    "    silhouette_avg, sample_silhouette_values = Silhouette(newweight, y)\n",
    "    Draw(silhouette_avg, sample_silhouette_values, y, k)\n",
    "    NewDraw(sample_silhouette_values, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "newweight, y  = lsakmeans(weight, k, 500, words)\n",
    "silhouette_avg, sample_silhouette_values = Silhouette(newweight, y)\n",
    "NewDraw(sample_silhouette_values, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NewDraw(sample_silhouette_values, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(8, 15):\n",
    "    newweight, y  = lsakmeans(weight, k, 500, words)\n",
    "    silhouette_avg, sample_silhouette_values = Silhouette(newweight, y)\n",
    "    Draw(silhouette_avg, sample_silhouette_values, y, k)\n",
    "    NewDraw(sample_silhouette_values, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [7175,7149,7120,7122,7114,7088,7070,6984,6827,6701,6624,6183]\n",
    "y = [8,9,10,11,12,13,14,20,30,40,50,100]\n",
    "plt.title('Métohd de coube')\n",
    "plt.ylabel('k-inertia')\n",
    "plt.xlabel('nombre de clusters k in [8,9, ... ,13,14,20,30,40,50,100]')\n",
    "plt.plot(y,x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k=50\n",
    "newweight, y  = lsakmeans(weight, k, 500, words)\n",
    "silhouette_avg, sample_silhouette_values = Silhouette(newweight, y)\n",
    "Draw(silhouette_avg, sample_silhouette_values, y, k)\n",
    "NewDraw(sample_silhouette_values, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k=30\n",
    "newweight, y  = lsakmeans(weight, k, 500, words)\n",
    "silhouette_avg, sample_silhouette_values = Silhouette(newweight, y)\n",
    "Draw(silhouette_avg, sample_silhouette_values, y, k)\n",
    "NewDraw(sample_silhouette_values, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=20\n",
    "newweight, y  = lsakmeans(weight, k, 500, words)\n",
    "silhouette_avg, sample_silhouette_values = Silhouette(newweight, y)\n",
    "Draw(silhouette_avg, sample_silhouette_values, y, k)\n",
    "NewDraw(sample_silhouette_values, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=40\n",
    "newweight, y  = lsakmeans(weight, k, 500, words)\n",
    "silhouette_avg, sample_silhouette_values = Silhouette(newweight, y)\n",
    "Draw(silhouette_avg, sample_silhouette_values, y, k)\n",
    "NewDraw(sample_silhouette_values, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=100\n",
    "newweight, y  = lsakmeans(weight, k, 500, words)\n",
    "silhouette_avg, sample_silhouette_values = Silhouette(newweight, y)\n",
    "Draw(silhouette_avg, sample_silhouette_values, y, k)\n",
    "NewDraw(sample_silhouette_values, y, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
