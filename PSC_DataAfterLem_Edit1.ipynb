{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import nltk.data\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class JSONObject:\n",
    "     def __init__(self, d):\n",
    "         self.__dict__ = d\n",
    "\n",
    "with open(\"articles_Accuracy.json\", \"r\") as read_file:\n",
    "    data = read_file.read()\n",
    "    obj = json.loads(data,object_hook=JSONObject)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopworddic = set(stopwords.words('french'))\n",
    "article=['a','au','aux','un','une','le','la','les','de','des','ce','cet','cette','ces','son','sa','ses','leur','leurs','mon','ma','mes','ton','ta','tes','notre','notres','votre','votres']\n",
    "pronom=['je','tu','il','elle','nous','vous','ils','elles','on','y','en','se']\n",
    "coordination=['mais','ou','et','donc','or','ni','car']\n",
    "v=['aller','vais','vas','va','allez','allions','vont']\n",
    "stopworddic.update(set(article),set(pronom),set(coordination),set(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = FrenchLefffLemmatizer()\n",
    "def lemma(word):\n",
    "    a=lemmatizer.lemmatize(word,'all')\n",
    "    b=[x[1] for x in a]\n",
    "    if 'nc' in b:\n",
    "        return lemmatizer.lemmatize(word,'n')\n",
    "    if 'v' in b:\n",
    "        return lemmatizer.lemmatize(word,'v')\n",
    "    return lemmatizer.lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "rawtokens=nltk.word_tokenize(obj[0].body_fr)\n",
    "metokens = [w.lower() for w in rawtokens if re.search('[a-zA-ZàâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ]+',w)]\n",
    "latokens=[]\n",
    "for str in metokens:\n",
    "    if(re.search(r'\\w+\\'',str)):\n",
    "        str=re.sub(r'\\w+\\'', '', str) \n",
    "    latokens.append(str)\n",
    "tokens = [w for w in latokens if w not in stopworddic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_data=[]\n",
    "for doc in obj :\n",
    "    if len(doc.body_fr)>2 :\n",
    "        text=doc.body_fr\n",
    "        text=nltk.word_tokenize(text)\n",
    "        text = [w.lower() for w in text if re.search('[a-zA-ZàâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ]+',w)]\n",
    "        latokens=[]\n",
    "        for str in text:\n",
    "            if(re.search(r'\\w+\\'',str)):\n",
    "                str=re.sub(r'\\w+\\'', '', str) \n",
    "            latokens.append(str)\n",
    "        tokens = [w for w in latokens if w not in stopworddic]\n",
    "        tokenized_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['groupe', 'défense', 'veut', 'accompagner', 'jeunes', 'pousses', 'françaises', 'haut', 'potentiel', 'ici', 'banque', 'complète', 'offre', 'travaille', 'insérer', 'écosystème', 'développement', 'entreprises', 'société', 'générale', 'met', 'quête', 'nouvelles', 'pépites', 'banque', 'défense', 'sert', 'déjà', 'start-up', 'dont', 'jugées', 'particulièrement', 'prometteuses', 'annoncé', 'mardi', 'souhaitait', 'séduire', 'supplémentaires', 'horizon', 'ensemble', 'territoire', 'jargon', 'banque', 'pépite', 'entreprise', 'déjà', 'réussi', 'première', 'levée', 'fonds', 'moins', 'euros', 'accompagnée', 'incubateur', 'phare', 'fonds', 'capital-risque', 'créée', 'idéalement', 'entrepreneur', 'coup', 'essai', 'peut', 'agir', 'entreprises', 'déjà', 'suivies', 'banque', 'doit', 'encore', 'grandir', 'comme', 'recrutements', 'externes', 'parvenir', 'banque', 'peaufine', 'arsenal', 'signer', 'lundi', 'prochain', 'partenariat', 'bpifrance', 'vue', 'notamment', 'faire', 'dialoguer', 'conseillers', 'réseau', 'bancaire', 'chargés', 'affaires', 'innovation', 'établissement', 'public', 'seul', 'bnp', 'paribas', 'heure', 'passé', 'tel', 'accord', 'bpifrance', 'deuxième', 'nouveauté', 'concerne', 'ensemble', 'jeunes', 'pousses', 'tech', 'bénéficier', 'offre', 'dédiée', 'baptisée', 'welcome', 'pack', 'startup', 'dernière', 'comprend', 'notamment', 'service', 'banque', 'mobile', 'moyens', 'paiement', 'encore', 'bilan', 'patrimonial', 'industrialiser', 'pratiques', 'démarche', 'groupe', 'bancaire', 'plus', 'loin', 'simple', 'offre', 'commerciale', 'vise', 'désormais', 'industrialiser', 'pratiques', 'déjà', 'place', 'depuis', 'plusieurs', 'années', 'animation', 'clientèle', 'si', 'particulière', 'compléter', 'entériner', 'dispositif', 'conseillers', 'référents', 'start-up', 'selon', 'villes', 'nombre', 'créateurs', 'suivre', 'rôle', 'occupé', 'plein', 'temps', 'façon', 'partielle', 'conseillers', 'clientèle', 'classiques', 'rôle', 'notamment', 'orienter', 'client', 'travers', 'ensemble', 'services', 'pourrait', 'apporter', 'groupe', 'enfin', 'huit', 'ambassadeurs', 'chargés', 'identifier', 'faire', 'connaître', 'direction', 'générale', 'entreprises', 'portefeuille', 'plus', 'pointe', 'plus', 'prometteuses', 'travail', 'animation', 'interne', 'partage', 'client', 'aussi', 'marque', 'fabrique', 'banque', 'logo', 'rouge', 'noir', 'mené', 'passé', 'type', 'démarches', 'développer', 'services', 'banque', 'privée', 'travail', 'réseau', 'banque', 'mène', 'aussi', 'externe', 'devenant', 'partenaire', 'bancaire', 'vingtaine', 'incubateurs', 'dont', 'station', 'f', 'wework', 'tremplin', 'horizon', 'espère', 'être', 'relation', 'incubateurs', 'premier', 'plan', 'france', 'entreprises', 'accompagnées', 'banque', 'pourraient', 'outre', 'être', 'accueillies', 'certains', 'centres', 'affaires', 'dunes', 'nouvel', 'immeuble', 'fontenay-sous-bois', 'démarche', 'banque', 'reconnaît', 'creux', 'pu', 'prendre', 'retard', 'thématiques', 'développement', 'innovation', 'réseau', 'partenaires', 'clients', 'tisser', 'prochaines', 'années', 'permettra', 'aussi', 'progresser', 'e.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Surface\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import models,corpora\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)\n",
    "lsi_model = models.LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Model:\n",
      "Topic #0: 0.009*\"the\" + 0.007*\"plus\" + 0.005*\"of\" + 0.004*\"and\" + 0.004*\"to\" + 0.004*\"comme\" + 0.004*\"luxembourg\" + 0.003*\"être\" + 0.003*\"france\" + 0.003*\"in\"\n",
      "Topic #1: 0.010*\"plus\" + 0.006*\"start-up\" + 0.004*\"entreprises\" + 0.004*\"fonds\" + 0.004*\"comme\" + 0.004*\"innovation\" + 0.004*\"aussi\" + 0.003*\"millions\" + 0.003*\"faire\" + 0.003*\"france\"\n",
      "Topic #2: 0.006*\"plus\" + 0.005*\"euros\" + 0.004*\"france\" + 0.003*\"millions\" + 0.003*\"groupe\" + 0.003*\"affaires\" + 0.002*\"aussi\" + 0.002*\"deux\" + 0.002*\"start-up\" + 0.002*\"paris\"\n",
      "Topic #3: 0.005*\"plus\" + 0.004*\"start-up\" + 0.004*\"aussi\" + 0.004*\"entreprises\" + 0.004*\"innovation\" + 0.004*\"comme\" + 0.004*\"entreprise\" + 0.003*\"fait\" + 0.003*\"être\" + 0.003*\"tout\"\n",
      "Topic #4: 0.007*\"plus\" + 0.005*\"entreprises\" + 0.005*\"entreprise\" + 0.005*\"aussi\" + 0.004*\"comme\" + 0.004*\"projet\" + 0.004*\"ans\" + 0.004*\"tout\" + 0.004*\"deux\" + 0.003*\"fait\"\n",
      "Topic #5: 0.006*\"start-up\" + 0.005*\"numérique\" + 0.005*\"plus\" + 0.004*\"paris\" + 0.004*\"entreprises\" + 0.004*\"innovation\" + 0.003*\"développement\" + 0.003*\"aussi\" + 0.003*\"ville\" + 0.003*\"être\"\n",
      "Topic #6: 0.010*\"plus\" + 0.006*\"start-up\" + 0.004*\"france\" + 0.004*\"aussi\" + 0.004*\"comme\" + 0.004*\"entreprises\" + 0.003*\"deux\" + 0.003*\"tout\" + 0.003*\"entre\" + 0.003*\"fait\"\n",
      "Topic #7: 0.009*\"h\" + 0.006*\"plus\" + 0.006*\"bordeaux\" + 0.005*\"entreprises\" + 0.003*\"euros\" + 0.003*\"aussi\" + 0.003*\"entreprise\" + 0.003*\"pépinière\" + 0.003*\"deux\" + 0.003*\"ans\"\n",
      "Topic #8: 0.008*\"entreprises\" + 0.007*\"plus\" + 0.005*\"projet\" + 0.004*\"projets\" + 0.004*\"start-up\" + 0.004*\"aussi\" + 0.003*\"deux\" + 0.003*\"ans\" + 0.003*\"être\" + 0.003*\"faire\"\n",
      "Topic #9: 0.004*\"start-up\" + 0.004*\"plus\" + 0.004*\"paris\" + 0.003*\"comme\" + 0.003*\"contact\" + 0.003*\"recherche\" + 0.002*\"euros\" + 0.002*\"tout\" + 0.002*\"fait\" + 0.002*\"entreprise\"\n",
      "====================\n",
      "LSI Model:\n",
      "Topic #0: 0.339*\"the\" + 0.323*\"plus\" + 0.197*\"of\" + 0.174*\"and\" + 0.154*\"to\" + 0.130*\"luxembourg\" + 0.129*\"entreprises\" + 0.117*\"comme\" + 0.113*\"in\" + 0.109*\"être\"\n",
      "Topic #1: 0.499*\"the\" + 0.282*\"of\" + 0.230*\"and\" + 0.222*\"to\" + 0.162*\"luxembourg\" + -0.159*\"bordeaux\" + 0.156*\"in\" + -0.125*\"entrepreneur\" + -0.118*\"dotation\" + -0.115*\"date\"\n",
      "Topic #2: 0.299*\"dotation\" + 0.295*\"entrepreneur\" + 0.294*\"date\" + 0.281*\"profil\" + 0.277*\"limite\" + 0.272*\"dossier\" + 0.270*\"dépôt\" + 0.268*\"inscrire\" + -0.177*\"plus\" + -0.155*\"bordeaux\"\n",
      "Topic #3: 0.470*\"bordeaux\" + -0.147*\"plus\" + 0.140*\"nouvelle-aquitaine\" + -0.133*\"comme\" + -0.126*\"start-up\" + 0.124*\"the\" + 0.122*\"aquitaine\" + -0.122*\"france\" + 0.114*\"gironde\" + -0.111*\"faire\"\n",
      "Topic #4: -0.454*\"euros\" + -0.398*\"millions\" + -0.184*\"résultat\" + -0.175*\"immobilier\" + -0.163*\"affaires\" + -0.160*\"opérationnel\" + -0.158*\"chiffre\" + -0.139*\"ifrs\" + -0.130*\"services\" + 0.126*\"bordeaux\"\n",
      "Topic #5: -0.436*\"campus\" + -0.251*\"opération\" + -0.233*\"universités\" + -0.229*\"état\" + -0.228*\"opérations\" + 0.157*\"euros\" + -0.147*\"université\" + -0.135*\"plan\" + -0.133*\"établissements\" + -0.126*\"projets\"\n",
      "Topic #6: -0.378*\"start-up\" + -0.308*\"recherche\" + -0.283*\"associé\" + 0.276*\"france\" + -0.209*\"contactez\" + 0.201*\"radio\" + -0.129*\"e\" + -0.123*\"business\" + -0.114*\"ans\" + 0.114*\"public\"\n",
      "Topic #7: 0.435*\"france\" + 0.333*\"radio\" + 0.222*\"recherche\" + -0.198*\"ans\" + 0.189*\"associé\" + 0.167*\"public\" + 0.148*\"contactez\" + 0.114*\"start-up\" + 0.110*\"publics\" + 0.106*\"médias\"\n",
      "Topic #8: 0.391*\"https\" + 0.372*\"r=9chkp3bakplifptles8oeribuqorq1cnygcyl5qe3rm\" + 0.372*\"//urldefense.proofpoint.com/v2/url\" + 0.372*\"d=dwmgaq\" + 0.372*\"e=\" + 0.372*\"c=sfymryylsugfxnyao2svzg\" + 0.136*\"m=xymsrsnoxkpcat84ordhmnwrikdkwtzhjvtrmf1itfg\" + 0.083*\"m=rn9ql8k1iao0yxt8qb7wvczofr9uicmxcgggplnqqx8\" + 0.071*\"h\" + 0.071*\"juin\"\n",
      "Topic #9: 0.365*\"the\" + -0.234*\"luxembourg\" + -0.178*\"and\" + -0.163*\"plus\" + 0.142*\"euros\" + -0.125*\"marché\" + 0.121*\"of\" + 0.118*\"paris\" + -0.113*\"services\" + 0.109*\"aussi\"\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(10):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 10))\n",
    " \n",
    "print(\"=\" * 20)\n",
    " \n",
    "print(\"LSI Model:\")\n",
    " \n",
    "for idx in range(10):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lsi_model.print_topic(idx, 10))\n",
    " \n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
